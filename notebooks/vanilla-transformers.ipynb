{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla Transformer with PyTorch\n",
    "\n",
    "This notebook is used to test the implementation of the vanilla transformer model from scratch using PyTorch. The model is saved under the `models` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path added to the sys path.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from path_utils import add_parent_path_to_sys_path\n",
    "# add the parent directory to the sys path so that we can import the models\n",
    "current_path = sys.path[0]\n",
    "add_parent_path_to_sys_path(current_path, verbose=False)\n",
    "\n",
    "# import the models\n",
    "from models.vanilla_transformers import Transformer\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer(\n",
      "  (encoder_embedding): Embedding(4096, 512)\n",
      "  (decoder_embedding): Embedding(4096, 512)\n",
      "  (positional_encoding): PositionalEncoding()\n",
      "  (transformer_encoder): ModuleList(\n",
      "    (0-5): 6 x TransformerEncoderLayer(\n",
      "      (self_attn): MultiHeadAttention(\n",
      "        (W_q): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (W_k): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (W_v): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (W_o): Linear(in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (feedforward): PositionWiseFeedForward(\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (transformer_decoder): ModuleList(\n",
      "    (0-5): 6 x TransformerDecoderLayer(\n",
      "      (masked_self_attn): MultiHeadAttention(\n",
      "        (W_q): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (W_k): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (W_v): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (W_o): Linear(in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (cross_attn): MultiHeadAttention(\n",
      "        (W_q): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (W_k): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (W_v): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (W_o): Linear(in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (feedforward): PositionWiseFeedForward(\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=512, out_features=4096, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# setting up the model\n",
    "source_vocab_size = 4096\n",
    "target_vocab_size = 4096\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "d_ff = 2048\n",
    "max_seq_length = 128\n",
    "dropout = 0.1\n",
    "\n",
    "transformer = Transformer(source_vocab_size,\n",
    "                          target_vocab_size,\n",
    "                          d_model,\n",
    "                          num_heads,\n",
    "                          num_layers,\n",
    "                          d_ff,\n",
    "                          max_seq_length,\n",
    "                          dropout)\n",
    "print(transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate fake data\n",
    "source_data = torch.randint(1, # low\n",
    "                            source_vocab_size, # high\n",
    "                            (1, max_seq_length), # size\n",
    "                            ) # (batch_size, max_seq_length)\n",
    "target_data = torch.randint(1, # low\n",
    "                            target_vocab_size, # high\n",
    "                            (1, max_seq_length), # size\n",
    "                            ) # (batch_size, max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 8.436713218688965\n",
      "Epoch: 2, Loss: 7.639357566833496\n",
      "Epoch: 3, Loss: 7.231591701507568\n",
      "Epoch: 4, Loss: 7.009538173675537\n",
      "Epoch: 5, Loss: 6.817314624786377\n",
      "Epoch: 6, Loss: 6.585111618041992\n",
      "Epoch: 7, Loss: 6.248195171356201\n",
      "Epoch: 8, Loss: 5.939205169677734\n",
      "Epoch: 9, Loss: 5.512218475341797\n",
      "Epoch: 10, Loss: 5.180314064025879\n"
     ]
    }
   ],
   "source": [
    "# setting up the loss function\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=0)\n",
    "# setting up the optimizer\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), # parameters\n",
    "                             lr=0.0001, # learning rate\n",
    "                             betas=(0.9, 0.98), # betas\n",
    "                             eps=1e-9, # eps\n",
    "                             weight_decay=0.0001, # weight decay\n",
    "                             )\n",
    "\n",
    "# training loop\n",
    "# running 10 epochs to test the training loop\n",
    "transformer.train()\n",
    "for epoch in range(1, 11):\n",
    "    # zero out the gradients\n",
    "    optimizer.zero_grad()\n",
    "    # forward pass\n",
    "    output = transformer(source_data, \n",
    "                         target_data[:, :-1], # input-output mismatch\n",
    "                        )\n",
    "    # calculate the loss\n",
    "    loss = criterion(output.contiguous().view(-1, target_vocab_size), # (batch_size * max_seq_length, target_vocab_size)\n",
    "                     target_data[:, 1:].contiguous().view(-1), # shift the target data by 1\n",
    "                     )\n",
    "    # backward pass\n",
    "    loss.backward()\n",
    "    # update the weights\n",
    "    optimizer.step()\n",
    "    # print the loss\n",
    "    print(f'Epoch: {epoch}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scratch-pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
